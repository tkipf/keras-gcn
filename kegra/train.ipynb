{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoshitaka-i/.conda/envs/tensor/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.layers import Input, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from kegra.layers.graph import GraphConvolution\n",
    "from kegra.utils import *\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "DATASET = 'cora'\n",
    "FILTER = 'localpool'  # 'chebyshev'\n",
    "MAX_DEGREE = 2  # maximum polynomial degree\n",
    "SYM_NORM = True  # symmetric (True) vs. left-only (False) normalization\n",
    "NB_EPOCH = 200\n",
    "PATIENCE = 10  # early stopping patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n",
      "Dataset has 2708 nodes, 5429 edges, 1433 features.\n",
      "Using local pooling filters...\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "X, A, y = load_data(dataset=DATASET)\n",
    "y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask = get_splits(y)\n",
    "\n",
    "# Normalize X\n",
    "X /= X.sum(1).reshape(-1, 1)\n",
    "\n",
    "if FILTER == 'localpool':\n",
    "    \"\"\" Local pooling filters (see 'renormalization trick' in Kipf & Welling, arXiv 2016) \"\"\"\n",
    "    print('Using local pooling filters...')\n",
    "    A_ = preprocess_adj(A, SYM_NORM)\n",
    "    support = 1\n",
    "    graph = [X, A_]\n",
    "    G = [Input(shape=(None, None), batch_shape=(None, None), sparse=True)]\n",
    "\n",
    "elif FILTER == 'chebyshev':\n",
    "    \"\"\" Chebyshev polynomial basis filters (Defferard et al., NIPS 2016)  \"\"\"\n",
    "    print('Using Chebyshev polynomial basis filters...')\n",
    "    L = normalized_laplacian(A, SYM_NORM)\n",
    "    L_scaled = rescale_laplacian(L)\n",
    "    T_k = chebyshev_polynomial(L_scaled, MAX_DEGREE)\n",
    "    support = MAX_DEGREE + 1\n",
    "    graph = [X]+T_k\n",
    "    G = [Input(shape=(None, None), batch_shape=(None, None), sparse=True) for _ in range(support)]\n",
    "\n",
    "else:\n",
    "    raise Exception('Invalid filter type.')\n",
    "\n",
    "X_in = Input(shape=(X.shape[1],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "# NOTE: We pass arguments for graph convolutional layers as a list of tensors.\n",
    "# This is somewhat hacky, more elegant options would require rewriting the Layer base class.\n",
    "H = Dropout(0.5)(X_in)\n",
    "H = GraphConvolution(16, support, activation='relu', kernel_regularizer=l2(5e-4))([H]+G)\n",
    "H = Dropout(0.5)(H)\n",
    "Y = GraphConvolution(y.shape[1], support, activation='softmax')([H]+G)\n",
    "\n",
    "# Compile model\n",
    "model = Model(inputs=[X_in]+G, outputs=Y)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01))\n",
    "\n",
    "# Helper variables for main training loop\n",
    "wait = 0\n",
    "preds = None\n",
    "best_val_loss = 99999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 1.9369 train_acc= 0.3643 val_loss= 1.9383 val_acc= 0.3633 time= 2.6145\n",
      "Epoch: 0002 train_loss= 1.9266 train_acc= 0.4214 val_loss= 1.9296 val_acc= 0.4100 time= 0.0214\n",
      "Epoch: 0003 train_loss= 1.9149 train_acc= 0.4643 val_loss= 1.9197 val_acc= 0.4367 time= 0.0223\n",
      "Epoch: 0004 train_loss= 1.9023 train_acc= 0.4643 val_loss= 1.9090 val_acc= 0.4500 time= 0.0217\n",
      "Epoch: 0005 train_loss= 1.8893 train_acc= 0.4643 val_loss= 1.8981 val_acc= 0.4100 time= 0.0254\n",
      "Epoch: 0006 train_loss= 1.8761 train_acc= 0.4500 val_loss= 1.8870 val_acc= 0.4033 time= 0.0262\n",
      "Epoch: 0007 train_loss= 1.8623 train_acc= 0.4500 val_loss= 1.8756 val_acc= 0.4033 time= 0.0289\n",
      "Epoch: 0008 train_loss= 1.8482 train_acc= 0.4500 val_loss= 1.8641 val_acc= 0.4100 time= 0.0247\n",
      "Epoch: 0009 train_loss= 1.8339 train_acc= 0.4500 val_loss= 1.8525 val_acc= 0.4167 time= 0.0230\n",
      "Epoch: 0010 train_loss= 1.8201 train_acc= 0.4500 val_loss= 1.8411 val_acc= 0.4100 time= 0.0234\n",
      "Epoch: 0011 train_loss= 1.8060 train_acc= 0.4500 val_loss= 1.8296 val_acc= 0.4067 time= 0.0220\n",
      "Epoch: 0012 train_loss= 1.7917 train_acc= 0.4429 val_loss= 1.8180 val_acc= 0.4067 time= 0.0227\n",
      "Epoch: 0013 train_loss= 1.7778 train_acc= 0.4357 val_loss= 1.8066 val_acc= 0.4000 time= 0.0232\n",
      "Epoch: 0014 train_loss= 1.7643 train_acc= 0.4143 val_loss= 1.7956 val_acc= 0.3867 time= 0.0271\n",
      "Epoch: 0015 train_loss= 1.7511 train_acc= 0.4143 val_loss= 1.7847 val_acc= 0.3800 time= 0.0272\n",
      "Epoch: 0016 train_loss= 1.7384 train_acc= 0.4071 val_loss= 1.7742 val_acc= 0.3667 time= 0.0228\n",
      "Epoch: 0017 train_loss= 1.7259 train_acc= 0.3929 val_loss= 1.7641 val_acc= 0.3633 time= 0.0280\n",
      "Epoch: 0018 train_loss= 1.7137 train_acc= 0.3714 val_loss= 1.7547 val_acc= 0.3633 time= 0.0272\n",
      "Epoch: 0019 train_loss= 1.7020 train_acc= 0.3571 val_loss= 1.7456 val_acc= 0.3633 time= 0.0282\n",
      "Epoch: 0020 train_loss= 1.6904 train_acc= 0.3571 val_loss= 1.7370 val_acc= 0.3633 time= 0.0269\n",
      "Epoch: 0021 train_loss= 1.6790 train_acc= 0.3714 val_loss= 1.7287 val_acc= 0.3633 time= 0.0263\n",
      "Epoch: 0022 train_loss= 1.6675 train_acc= 0.3786 val_loss= 1.7205 val_acc= 0.3633 time= 0.0259\n",
      "Epoch: 0023 train_loss= 1.6556 train_acc= 0.3857 val_loss= 1.7124 val_acc= 0.3633 time= 0.0305\n",
      "Epoch: 0024 train_loss= 1.6437 train_acc= 0.4000 val_loss= 1.7043 val_acc= 0.3667 time= 0.0300\n",
      "Epoch: 0025 train_loss= 1.6315 train_acc= 0.4143 val_loss= 1.6961 val_acc= 0.3700 time= 0.0260\n",
      "Epoch: 0026 train_loss= 1.6191 train_acc= 0.4143 val_loss= 1.6877 val_acc= 0.3800 time= 0.0295\n",
      "Epoch: 0027 train_loss= 1.6063 train_acc= 0.4143 val_loss= 1.6791 val_acc= 0.3867 time= 0.0243\n",
      "Epoch: 0028 train_loss= 1.5933 train_acc= 0.4429 val_loss= 1.6703 val_acc= 0.3967 time= 0.0245\n",
      "Epoch: 0029 train_loss= 1.5804 train_acc= 0.4429 val_loss= 1.6617 val_acc= 0.4067 time= 0.0297\n",
      "Epoch: 0030 train_loss= 1.5676 train_acc= 0.4429 val_loss= 1.6529 val_acc= 0.4133 time= 0.0283\n",
      "Epoch: 0031 train_loss= 1.5549 train_acc= 0.4643 val_loss= 1.6442 val_acc= 0.4167 time= 0.0280\n",
      "Epoch: 0032 train_loss= 1.5422 train_acc= 0.4643 val_loss= 1.6352 val_acc= 0.4167 time= 0.0285\n",
      "Epoch: 0033 train_loss= 1.5294 train_acc= 0.4643 val_loss= 1.6261 val_acc= 0.4167 time= 0.0298\n",
      "Epoch: 0034 train_loss= 1.5165 train_acc= 0.4643 val_loss= 1.6169 val_acc= 0.4200 time= 0.0280\n",
      "Epoch: 0035 train_loss= 1.5035 train_acc= 0.4714 val_loss= 1.6074 val_acc= 0.4200 time= 0.0265\n",
      "Epoch: 0036 train_loss= 1.4905 train_acc= 0.4714 val_loss= 1.5979 val_acc= 0.4267 time= 0.0285\n",
      "Epoch: 0037 train_loss= 1.4775 train_acc= 0.4857 val_loss= 1.5884 val_acc= 0.4400 time= 0.0294\n",
      "Epoch: 0038 train_loss= 1.4646 train_acc= 0.5000 val_loss= 1.5790 val_acc= 0.4433 time= 0.0278\n",
      "Epoch: 0039 train_loss= 1.4517 train_acc= 0.5000 val_loss= 1.5698 val_acc= 0.4433 time= 0.0290\n",
      "Epoch: 0040 train_loss= 1.4389 train_acc= 0.5000 val_loss= 1.5603 val_acc= 0.4500 time= 0.0263\n",
      "Epoch: 0041 train_loss= 1.4259 train_acc= 0.5000 val_loss= 1.5508 val_acc= 0.4633 time= 0.0223\n",
      "Epoch: 0042 train_loss= 1.4129 train_acc= 0.5071 val_loss= 1.5417 val_acc= 0.4700 time= 0.0229\n",
      "Epoch: 0043 train_loss= 1.3997 train_acc= 0.5214 val_loss= 1.5325 val_acc= 0.4767 time= 0.0220\n",
      "Epoch: 0044 train_loss= 1.3866 train_acc= 0.5214 val_loss= 1.5231 val_acc= 0.4767 time= 0.0223\n",
      "Epoch: 0045 train_loss= 1.3737 train_acc= 0.5214 val_loss= 1.5139 val_acc= 0.4800 time= 0.0260\n",
      "Epoch: 0046 train_loss= 1.3607 train_acc= 0.5286 val_loss= 1.5050 val_acc= 0.4867 time= 0.0299\n",
      "Epoch: 0047 train_loss= 1.3476 train_acc= 0.5357 val_loss= 1.4960 val_acc= 0.4867 time= 0.0292\n",
      "Epoch: 0048 train_loss= 1.3343 train_acc= 0.5500 val_loss= 1.4868 val_acc= 0.4967 time= 0.0239\n",
      "Epoch: 0049 train_loss= 1.3210 train_acc= 0.5571 val_loss= 1.4774 val_acc= 0.5033 time= 0.0231\n",
      "Epoch: 0050 train_loss= 1.3079 train_acc= 0.5714 val_loss= 1.4679 val_acc= 0.5100 time= 0.0242\n",
      "Epoch: 0051 train_loss= 1.2948 train_acc= 0.5857 val_loss= 1.4581 val_acc= 0.5167 time= 0.0226\n",
      "Epoch: 0052 train_loss= 1.2820 train_acc= 0.5929 val_loss= 1.4483 val_acc= 0.5233 time= 0.0234\n",
      "Epoch: 0053 train_loss= 1.2696 train_acc= 0.6000 val_loss= 1.4387 val_acc= 0.5267 time= 0.0237\n",
      "Epoch: 0054 train_loss= 1.2573 train_acc= 0.6286 val_loss= 1.4297 val_acc= 0.5367 time= 0.0227\n",
      "Epoch: 0055 train_loss= 1.2452 train_acc= 0.6357 val_loss= 1.4208 val_acc= 0.5400 time= 0.0271\n",
      "Epoch: 0056 train_loss= 1.2330 train_acc= 0.6429 val_loss= 1.4120 val_acc= 0.5600 time= 0.0292\n",
      "Epoch: 0057 train_loss= 1.2207 train_acc= 0.6571 val_loss= 1.4033 val_acc= 0.5700 time= 0.0312\n",
      "Epoch: 0058 train_loss= 1.2083 train_acc= 0.6643 val_loss= 1.3947 val_acc= 0.5800 time= 0.0296\n",
      "Epoch: 0059 train_loss= 1.1961 train_acc= 0.7071 val_loss= 1.3858 val_acc= 0.5933 time= 0.0289\n",
      "Epoch: 0060 train_loss= 1.1841 train_acc= 0.7214 val_loss= 1.3768 val_acc= 0.6067 time= 0.0243\n",
      "Epoch: 0061 train_loss= 1.1722 train_acc= 0.7286 val_loss= 1.3678 val_acc= 0.6067 time= 0.0239\n",
      "Epoch: 0062 train_loss= 1.1604 train_acc= 0.7286 val_loss= 1.3590 val_acc= 0.6133 time= 0.0243\n",
      "Epoch: 0063 train_loss= 1.1487 train_acc= 0.7429 val_loss= 1.3503 val_acc= 0.6200 time= 0.0289\n",
      "Epoch: 0064 train_loss= 1.1372 train_acc= 0.7571 val_loss= 1.3414 val_acc= 0.6267 time= 0.0281\n",
      "Epoch: 0065 train_loss= 1.1258 train_acc= 0.7571 val_loss= 1.3325 val_acc= 0.6367 time= 0.0283\n",
      "Epoch: 0066 train_loss= 1.1145 train_acc= 0.7643 val_loss= 1.3236 val_acc= 0.6433 time= 0.0288\n",
      "Epoch: 0067 train_loss= 1.1031 train_acc= 0.7929 val_loss= 1.3143 val_acc= 0.6467 time= 0.0291\n",
      "Epoch: 0068 train_loss= 1.0918 train_acc= 0.7929 val_loss= 1.3049 val_acc= 0.6633 time= 0.0288\n",
      "Epoch: 0069 train_loss= 1.0806 train_acc= 0.8071 val_loss= 1.2957 val_acc= 0.6667 time= 0.0273\n",
      "Epoch: 0070 train_loss= 1.0698 train_acc= 0.8214 val_loss= 1.2868 val_acc= 0.6767 time= 0.0261\n",
      "Epoch: 0071 train_loss= 1.0592 train_acc= 0.8214 val_loss= 1.2786 val_acc= 0.6867 time= 0.0257\n",
      "Epoch: 0072 train_loss= 1.0489 train_acc= 0.8214 val_loss= 1.2711 val_acc= 0.6900 time= 0.0250\n",
      "Epoch: 0073 train_loss= 1.0389 train_acc= 0.8214 val_loss= 1.2636 val_acc= 0.6933 time= 0.0243\n",
      "Epoch: 0074 train_loss= 1.0291 train_acc= 0.8214 val_loss= 1.2560 val_acc= 0.6933 time= 0.0240\n",
      "Epoch: 0075 train_loss= 1.0194 train_acc= 0.8214 val_loss= 1.2487 val_acc= 0.6933 time= 0.0250\n",
      "Epoch: 0076 train_loss= 1.0099 train_acc= 0.8214 val_loss= 1.2415 val_acc= 0.6933 time= 0.0221\n",
      "Epoch: 0077 train_loss= 1.0002 train_acc= 0.8214 val_loss= 1.2344 val_acc= 0.7067 time= 0.0265\n",
      "Epoch: 0078 train_loss= 0.9908 train_acc= 0.8214 val_loss= 1.2273 val_acc= 0.7033 time= 0.0261\n",
      "Epoch: 0079 train_loss= 0.9816 train_acc= 0.8214 val_loss= 1.2206 val_acc= 0.7267 time= 0.0274\n",
      "Epoch: 0080 train_loss= 0.9728 train_acc= 0.8286 val_loss= 1.2141 val_acc= 0.7367 time= 0.0276\n",
      "Epoch: 0081 train_loss= 0.9638 train_acc= 0.8286 val_loss= 1.2076 val_acc= 0.7400 time= 0.0273\n",
      "Epoch: 0082 train_loss= 0.9550 train_acc= 0.8286 val_loss= 1.2012 val_acc= 0.7433 time= 0.0263\n",
      "Epoch: 0083 train_loss= 0.9459 train_acc= 0.8357 val_loss= 1.1942 val_acc= 0.7500 time= 0.0238\n",
      "Epoch: 0084 train_loss= 0.9370 train_acc= 0.8429 val_loss= 1.1874 val_acc= 0.7567 time= 0.0219\n",
      "Epoch: 0085 train_loss= 0.9283 train_acc= 0.8500 val_loss= 1.1805 val_acc= 0.7567 time= 0.0232\n",
      "Epoch: 0086 train_loss= 0.9194 train_acc= 0.8571 val_loss= 1.1733 val_acc= 0.7567 time= 0.0299\n",
      "Epoch: 0087 train_loss= 0.9107 train_acc= 0.8571 val_loss= 1.1659 val_acc= 0.7600 time= 0.0301\n",
      "Epoch: 0088 train_loss= 0.9025 train_acc= 0.8571 val_loss= 1.1589 val_acc= 0.7533 time= 0.0295\n",
      "Epoch: 0089 train_loss= 0.8948 train_acc= 0.8714 val_loss= 1.1522 val_acc= 0.7567 time= 0.0306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0090 train_loss= 0.8877 train_acc= 0.8786 val_loss= 1.1461 val_acc= 0.7567 time= 0.0290\n",
      "Epoch: 0091 train_loss= 0.8807 train_acc= 0.8786 val_loss= 1.1399 val_acc= 0.7500 time= 0.0288\n",
      "Epoch: 0092 train_loss= 0.8739 train_acc= 0.8714 val_loss= 1.1334 val_acc= 0.7467 time= 0.0296\n",
      "Epoch: 0093 train_loss= 0.8669 train_acc= 0.8643 val_loss= 1.1271 val_acc= 0.7533 time= 0.0284\n",
      "Epoch: 0094 train_loss= 0.8593 train_acc= 0.8643 val_loss= 1.1211 val_acc= 0.7500 time= 0.0280\n",
      "Epoch: 0095 train_loss= 0.8522 train_acc= 0.8714 val_loss= 1.1161 val_acc= 0.7467 time= 0.0292\n",
      "Epoch: 0096 train_loss= 0.8453 train_acc= 0.8786 val_loss= 1.1114 val_acc= 0.7567 time= 0.0292\n",
      "Epoch: 0097 train_loss= 0.8386 train_acc= 0.8786 val_loss= 1.1067 val_acc= 0.7567 time= 0.0306\n",
      "Epoch: 0098 train_loss= 0.8321 train_acc= 0.8786 val_loss= 1.1023 val_acc= 0.7600 time= 0.0276\n",
      "Epoch: 0099 train_loss= 0.8258 train_acc= 0.8857 val_loss= 1.0981 val_acc= 0.7633 time= 0.0273\n",
      "Epoch: 0100 train_loss= 0.8195 train_acc= 0.8929 val_loss= 1.0937 val_acc= 0.7633 time= 0.0258\n",
      "Epoch: 0101 train_loss= 0.8128 train_acc= 0.8857 val_loss= 1.0879 val_acc= 0.7667 time= 0.0264\n",
      "Epoch: 0102 train_loss= 0.8062 train_acc= 0.8857 val_loss= 1.0823 val_acc= 0.7633 time= 0.0281\n",
      "Epoch: 0103 train_loss= 0.7999 train_acc= 0.8857 val_loss= 1.0764 val_acc= 0.7633 time= 0.0290\n",
      "Epoch: 0104 train_loss= 0.7940 train_acc= 0.9000 val_loss= 1.0710 val_acc= 0.7667 time= 0.0304\n",
      "Epoch: 0105 train_loss= 0.7882 train_acc= 0.9071 val_loss= 1.0661 val_acc= 0.7733 time= 0.0291\n",
      "Epoch: 0106 train_loss= 0.7823 train_acc= 0.9071 val_loss= 1.0618 val_acc= 0.7800 time= 0.0276\n",
      "Epoch: 0107 train_loss= 0.7766 train_acc= 0.9071 val_loss= 1.0582 val_acc= 0.7767 time= 0.0221\n",
      "Epoch: 0108 train_loss= 0.7709 train_acc= 0.9071 val_loss= 1.0547 val_acc= 0.7733 time= 0.0217\n",
      "Epoch: 0109 train_loss= 0.7652 train_acc= 0.9071 val_loss= 1.0514 val_acc= 0.7700 time= 0.0217\n",
      "Epoch: 0110 train_loss= 0.7599 train_acc= 0.9071 val_loss= 1.0476 val_acc= 0.7733 time= 0.0218\n",
      "Epoch: 0111 train_loss= 0.7549 train_acc= 0.9000 val_loss= 1.0440 val_acc= 0.7700 time= 0.0220\n",
      "Epoch: 0112 train_loss= 0.7499 train_acc= 0.9000 val_loss= 1.0399 val_acc= 0.7700 time= 0.0239\n",
      "Epoch: 0113 train_loss= 0.7445 train_acc= 0.9000 val_loss= 1.0354 val_acc= 0.7700 time= 0.0246\n",
      "Epoch: 0114 train_loss= 0.7386 train_acc= 0.9071 val_loss= 1.0317 val_acc= 0.7700 time= 0.0284\n",
      "Epoch: 0115 train_loss= 0.7334 train_acc= 0.9071 val_loss= 1.0288 val_acc= 0.7733 time= 0.0299\n",
      "Epoch: 0116 train_loss= 0.7290 train_acc= 0.9071 val_loss= 1.0268 val_acc= 0.7767 time= 0.0278\n",
      "Epoch: 0117 train_loss= 0.7240 train_acc= 0.9143 val_loss= 1.0228 val_acc= 0.7800 time= 0.0264\n",
      "Epoch: 0118 train_loss= 0.7180 train_acc= 0.9143 val_loss= 1.0167 val_acc= 0.7800 time= 0.0229\n",
      "Epoch: 0119 train_loss= 0.7122 train_acc= 0.9071 val_loss= 1.0095 val_acc= 0.7833 time= 0.0234\n",
      "Epoch: 0120 train_loss= 0.7080 train_acc= 0.9000 val_loss= 1.0036 val_acc= 0.7900 time= 0.0263\n",
      "Epoch: 0121 train_loss= 0.7050 train_acc= 0.9000 val_loss= 0.9995 val_acc= 0.7867 time= 0.0266\n",
      "Epoch: 0122 train_loss= 0.7027 train_acc= 0.9000 val_loss= 0.9964 val_acc= 0.7933 time= 0.0307\n",
      "Epoch: 0123 train_loss= 0.6982 train_acc= 0.9000 val_loss= 0.9930 val_acc= 0.7967 time= 0.0298\n",
      "Epoch: 0124 train_loss= 0.6919 train_acc= 0.9000 val_loss= 0.9888 val_acc= 0.7900 time= 0.0306\n",
      "Epoch: 0125 train_loss= 0.6857 train_acc= 0.9000 val_loss= 0.9858 val_acc= 0.7833 time= 0.0295\n",
      "Epoch: 0126 train_loss= 0.6811 train_acc= 0.9071 val_loss= 0.9845 val_acc= 0.7800 time= 0.0286\n",
      "Epoch: 0127 train_loss= 0.6790 train_acc= 0.9214 val_loss= 0.9853 val_acc= 0.7833 time= 0.0265\n",
      "Epoch: 0128 train_loss= 0.6756 train_acc= 0.9214 val_loss= 0.9834 val_acc= 0.7833 time= 0.0249\n",
      "Epoch: 0129 train_loss= 0.6711 train_acc= 0.9214 val_loss= 0.9809 val_acc= 0.7867 time= 0.0281\n",
      "Epoch: 0130 train_loss= 0.6649 train_acc= 0.9214 val_loss= 0.9754 val_acc= 0.7833 time= 0.0292\n",
      "Epoch: 0131 train_loss= 0.6586 train_acc= 0.9214 val_loss= 0.9691 val_acc= 0.7833 time= 0.0308\n",
      "Epoch: 0132 train_loss= 0.6546 train_acc= 0.9071 val_loss= 0.9654 val_acc= 0.7867 time= 0.0307\n",
      "Epoch: 0133 train_loss= 0.6518 train_acc= 0.9000 val_loss= 0.9630 val_acc= 0.7900 time= 0.0283\n",
      "Epoch: 0134 train_loss= 0.6490 train_acc= 0.9000 val_loss= 0.9612 val_acc= 0.7833 time= 0.0282\n",
      "Epoch: 0135 train_loss= 0.6447 train_acc= 0.9000 val_loss= 0.9583 val_acc= 0.7900 time= 0.0278\n",
      "Epoch: 0136 train_loss= 0.6403 train_acc= 0.9071 val_loss= 0.9553 val_acc= 0.7867 time= 0.0289\n",
      "Epoch: 0137 train_loss= 0.6363 train_acc= 0.9214 val_loss= 0.9530 val_acc= 0.7800 time= 0.0303\n",
      "Epoch: 0138 train_loss= 0.6338 train_acc= 0.9214 val_loss= 0.9526 val_acc= 0.7833 time= 0.0286\n",
      "Epoch: 0139 train_loss= 0.6308 train_acc= 0.9286 val_loss= 0.9502 val_acc= 0.7833 time= 0.0267\n",
      "Epoch: 0140 train_loss= 0.6280 train_acc= 0.9286 val_loss= 0.9478 val_acc= 0.7867 time= 0.0250\n",
      "Epoch: 0141 train_loss= 0.6242 train_acc= 0.9286 val_loss= 0.9440 val_acc= 0.7900 time= 0.0263\n",
      "Epoch: 0142 train_loss= 0.6202 train_acc= 0.9214 val_loss= 0.9394 val_acc= 0.7867 time= 0.0300\n",
      "Epoch: 0143 train_loss= 0.6160 train_acc= 0.9214 val_loss= 0.9349 val_acc= 0.7867 time= 0.0283\n",
      "Epoch: 0144 train_loss= 0.6123 train_acc= 0.9143 val_loss= 0.9311 val_acc= 0.7833 time= 0.0266\n",
      "Epoch: 0145 train_loss= 0.6088 train_acc= 0.9286 val_loss= 0.9279 val_acc= 0.7833 time= 0.0274\n",
      "Epoch: 0146 train_loss= 0.6052 train_acc= 0.9286 val_loss= 0.9239 val_acc= 0.7833 time= 0.0270\n",
      "Epoch: 0147 train_loss= 0.6017 train_acc= 0.9357 val_loss= 0.9206 val_acc= 0.7833 time= 0.0292\n",
      "Epoch: 0148 train_loss= 0.5982 train_acc= 0.9429 val_loss= 0.9176 val_acc= 0.7833 time= 0.0283\n",
      "Epoch: 0149 train_loss= 0.5951 train_acc= 0.9429 val_loss= 0.9149 val_acc= 0.7867 time= 0.0293\n",
      "Epoch: 0150 train_loss= 0.5920 train_acc= 0.9429 val_loss= 0.9127 val_acc= 0.7867 time= 0.0300\n",
      "Epoch: 0151 train_loss= 0.5891 train_acc= 0.9429 val_loss= 0.9109 val_acc= 0.7900 time= 0.0296\n",
      "Epoch: 0152 train_loss= 0.5861 train_acc= 0.9429 val_loss= 0.9094 val_acc= 0.7933 time= 0.0279\n",
      "Epoch: 0153 train_loss= 0.5830 train_acc= 0.9429 val_loss= 0.9066 val_acc= 0.7933 time= 0.0268\n",
      "Epoch: 0154 train_loss= 0.5798 train_acc= 0.9429 val_loss= 0.9031 val_acc= 0.7933 time= 0.0244\n",
      "Epoch: 0155 train_loss= 0.5769 train_acc= 0.9429 val_loss= 0.8997 val_acc= 0.7900 time= 0.0257\n",
      "Epoch: 0156 train_loss= 0.5739 train_acc= 0.9429 val_loss= 0.8978 val_acc= 0.7933 time= 0.0239\n",
      "Epoch: 0157 train_loss= 0.5709 train_acc= 0.9429 val_loss= 0.8962 val_acc= 0.7967 time= 0.0241\n",
      "Epoch: 0158 train_loss= 0.5678 train_acc= 0.9429 val_loss= 0.8950 val_acc= 0.7900 time= 0.0246\n",
      "Epoch: 0159 train_loss= 0.5646 train_acc= 0.9429 val_loss= 0.8932 val_acc= 0.7900 time= 0.0280\n",
      "Epoch: 0160 train_loss= 0.5616 train_acc= 0.9429 val_loss= 0.8910 val_acc= 0.7933 time= 0.0285\n",
      "Epoch: 0161 train_loss= 0.5586 train_acc= 0.9429 val_loss= 0.8892 val_acc= 0.7867 time= 0.0272\n",
      "Epoch: 0162 train_loss= 0.5556 train_acc= 0.9429 val_loss= 0.8873 val_acc= 0.7867 time= 0.0283\n",
      "Epoch: 0163 train_loss= 0.5525 train_acc= 0.9429 val_loss= 0.8853 val_acc= 0.7867 time= 0.0278\n",
      "Epoch: 0164 train_loss= 0.5499 train_acc= 0.9357 val_loss= 0.8840 val_acc= 0.7867 time= 0.0268\n",
      "Epoch: 0165 train_loss= 0.5468 train_acc= 0.9429 val_loss= 0.8823 val_acc= 0.7867 time= 0.0252\n",
      "Epoch: 0166 train_loss= 0.5436 train_acc= 0.9429 val_loss= 0.8800 val_acc= 0.7900 time= 0.0276\n",
      "Epoch: 0167 train_loss= 0.5410 train_acc= 0.9429 val_loss= 0.8782 val_acc= 0.7900 time= 0.0246\n",
      "Epoch: 0168 train_loss= 0.5378 train_acc= 0.9429 val_loss= 0.8752 val_acc= 0.7900 time= 0.0279\n",
      "Epoch: 0169 train_loss= 0.5346 train_acc= 0.9429 val_loss= 0.8710 val_acc= 0.7900 time= 0.0259\n",
      "Epoch: 0170 train_loss= 0.5318 train_acc= 0.9429 val_loss= 0.8675 val_acc= 0.7933 time= 0.0248\n",
      "Epoch: 0171 train_loss= 0.5289 train_acc= 0.9429 val_loss= 0.8656 val_acc= 0.7933 time= 0.0238\n",
      "Epoch: 0172 train_loss= 0.5262 train_acc= 0.9500 val_loss= 0.8643 val_acc= 0.8000 time= 0.0239\n",
      "Epoch: 0173 train_loss= 0.5240 train_acc= 0.9500 val_loss= 0.8632 val_acc= 0.8000 time= 0.0239\n",
      "Epoch: 0174 train_loss= 0.5220 train_acc= 0.9500 val_loss= 0.8618 val_acc= 0.8000 time= 0.0232\n",
      "Epoch: 0175 train_loss= 0.5197 train_acc= 0.9500 val_loss= 0.8603 val_acc= 0.8033 time= 0.0240\n",
      "Epoch: 0176 train_loss= 0.5174 train_acc= 0.9500 val_loss= 0.8583 val_acc= 0.8033 time= 0.0284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0177 train_loss= 0.5142 train_acc= 0.9500 val_loss= 0.8560 val_acc= 0.8067 time= 0.0290\n",
      "Epoch: 0178 train_loss= 0.5110 train_acc= 0.9500 val_loss= 0.8536 val_acc= 0.8033 time= 0.0289\n",
      "Epoch: 0179 train_loss= 0.5081 train_acc= 0.9500 val_loss= 0.8519 val_acc= 0.8033 time= 0.0311\n",
      "Epoch: 0180 train_loss= 0.5054 train_acc= 0.9500 val_loss= 0.8496 val_acc= 0.8000 time= 0.0313\n",
      "Epoch: 0181 train_loss= 0.5030 train_acc= 0.9500 val_loss= 0.8483 val_acc= 0.7967 time= 0.0282\n",
      "Epoch: 0182 train_loss= 0.5008 train_acc= 0.9500 val_loss= 0.8483 val_acc= 0.7967 time= 0.0291\n",
      "Epoch: 0183 train_loss= 0.4986 train_acc= 0.9500 val_loss= 0.8482 val_acc= 0.7967 time= 0.0265\n",
      "Epoch: 0184 train_loss= 0.4965 train_acc= 0.9500 val_loss= 0.8464 val_acc= 0.7967 time= 0.0255\n",
      "Epoch: 0185 train_loss= 0.4942 train_acc= 0.9500 val_loss= 0.8437 val_acc= 0.7967 time= 0.0270\n",
      "Epoch: 0186 train_loss= 0.4923 train_acc= 0.9500 val_loss= 0.8416 val_acc= 0.8000 time= 0.0289\n",
      "Epoch: 0187 train_loss= 0.4904 train_acc= 0.9500 val_loss= 0.8389 val_acc= 0.8000 time= 0.0291\n",
      "Epoch: 0188 train_loss= 0.4889 train_acc= 0.9500 val_loss= 0.8364 val_acc= 0.8033 time= 0.0292\n",
      "Epoch: 0189 train_loss= 0.4882 train_acc= 0.9429 val_loss= 0.8345 val_acc= 0.8033 time= 0.0264\n",
      "Epoch: 0190 train_loss= 0.4872 train_acc= 0.9429 val_loss= 0.8329 val_acc= 0.7933 time= 0.0242\n",
      "Epoch: 0191 train_loss= 0.4853 train_acc= 0.9500 val_loss= 0.8315 val_acc= 0.8000 time= 0.0265\n",
      "Epoch: 0192 train_loss= 0.4833 train_acc= 0.9500 val_loss= 0.8299 val_acc= 0.8000 time= 0.0307\n",
      "Epoch: 0193 train_loss= 0.4805 train_acc= 0.9500 val_loss= 0.8282 val_acc= 0.7967 time= 0.0287\n",
      "Epoch: 0194 train_loss= 0.4780 train_acc= 0.9500 val_loss= 0.8268 val_acc= 0.8000 time= 0.0249\n",
      "Epoch: 0195 train_loss= 0.4759 train_acc= 0.9500 val_loss= 0.8263 val_acc= 0.8033 time= 0.0233\n",
      "Epoch: 0196 train_loss= 0.4741 train_acc= 0.9500 val_loss= 0.8257 val_acc= 0.8033 time= 0.0233\n",
      "Epoch: 0197 train_loss= 0.4718 train_acc= 0.9500 val_loss= 0.8241 val_acc= 0.8033 time= 0.0262\n",
      "Epoch: 0198 train_loss= 0.4694 train_acc= 0.9500 val_loss= 0.8222 val_acc= 0.8100 time= 0.0230\n",
      "Epoch: 0199 train_loss= 0.4672 train_acc= 0.9500 val_loss= 0.8195 val_acc= 0.8100 time= 0.0226\n",
      "Epoch: 0200 train_loss= 0.4655 train_acc= 0.9500 val_loss= 0.8179 val_acc= 0.8067 time= 0.0280\n"
     ]
    }
   ],
   "source": [
    "# Fit\n",
    "for epoch in range(1, NB_EPOCH+1):\n",
    "\n",
    "    # Log wall-clock time\n",
    "    t = time.time()\n",
    "\n",
    "    # Single training iteration (we mask nodes without labels for loss calculation)\n",
    "    model.fit(graph, y_train, sample_weight=train_mask,\n",
    "              batch_size=A.shape[0], epochs=1, shuffle=False, verbose=0)\n",
    "\n",
    "    # Predict on full dataset\n",
    "    preds = model.predict(graph, batch_size=A.shape[0])\n",
    "\n",
    "    # Train / validation scores\n",
    "    train_val_loss, train_val_acc = evaluate_preds(preds, [y_train, y_val],\n",
    "                                                   [idx_train, idx_val])\n",
    "    print(\"Epoch: {:04d}\".format(epoch),\n",
    "          \"train_loss= {:.4f}\".format(train_val_loss[0]),\n",
    "          \"train_acc= {:.4f}\".format(train_val_acc[0]),\n",
    "          \"val_loss= {:.4f}\".format(train_val_loss[1]),\n",
    "          \"val_acc= {:.4f}\".format(train_val_acc[1]),\n",
    "          \"time= {:.4f}\".format(time.time() - t))\n",
    "\n",
    "    # Early stopping\n",
    "    if train_val_loss[1] < best_val_loss:\n",
    "        best_val_loss = train_val_loss[1]\n",
    "        wait = 0\n",
    "    else:\n",
    "        if wait >= PATIENCE:\n",
    "            print('Epoch {}: early stopping'.format(epoch))\n",
    "            break\n",
    "        wait += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.8773 accuracy= 0.7960\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test_loss, test_acc = evaluate_preds(preds, [y_test], [idx_test])\n",
    "print(\"Test set results:\",\n",
    "      \"loss= {:.4f}\".format(test_loss[0]),\n",
    "      \"accuracy= {:.4f}\".format(test_acc[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
